# Healthcare Revenue Cycle Management â€“ GCP Data Engineering Project

## Overview

This project implements an end-to-end Data Engineering pipeline on Google Cloud Platform (GCP) for the Healthcare Revenue Cycle Management (RCM) domain.

The solution ingests data from multiple healthcare sources, processes it using a Medallion Architecture (Bronze â†’ Silver â†’ Gold), and delivers analytics-ready fact and dimension tables in BigQuery to support KPI reporting.

The entire project was designed with Cloud Composer (Airflow) as the orchestration core, ensuring modular workflows, clear task dependencies, and production-style scheduling patterns.

---

## Architecture Summary

**Sources**
- Cloud SQL (Hospital A & B â€“ EMR data)
- Claims flat files (GCS)
- Public APIs (CPT, ICD, NPI)

**Processing**
- PySpark jobs on Dataproc (data ingestion and raw processing)
- BigQuery transformations (Bronze, Silver, Gold layers)

**Orchestration**
- Cloud Composer (Airflow DAGs)
- Parent DAG coordinates the full workflow
- PySpark DAG handles ingestion jobs
- BigQuery DAG executes transformation layers

**CI/CD**
- Cloud Build trigger configured on repository updates
- Any change pushed to Git automatically:
  - Deploys DAGs to Composer
  - Uploads required data/SQL files to the Composer bucket
- Enables automated deployment and version-controlled workflows

---

## ğŸ“‚ Project Structure

```text
GCP-HEALTHCARE-PROJECT-MAIN/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“ BQ/                # BigQuery SQL scripts (Bronze, Silver, Gold layers)
â”‚   â”œâ”€â”€ ğŸ“ claims/            # Raw Hospital claim CSV files
â”‚   â”œâ”€â”€ ğŸ“ configs/           # DDLs and load metadata
â”‚   â”œâ”€â”€ ğŸ“ cptcodes/          # CPT reference data
â”‚   â””â”€â”€ ğŸ“ EMR/               # EMR data for Hospital A & B
â”‚       â”œâ”€â”€ ğŸ“ hospital-a/    # Source-specific DDLs and CSVs
â”‚       â””â”€â”€ ğŸ“ hospital-b/    # Source-specific DDLs and CSVs
â”œâ”€â”€ ğŸ“ INGESTION/             # PySpark & Python ETL ingestion scripts
â”‚   â”œâ”€â”€ claims.py
â”‚   â”œâ”€â”€ cpt_codes.py
â”‚   â”œâ”€â”€ hospitalA_mysqlToLanding.py
â”‚   â”œâ”€â”€ hospitalB_mysqlToLanding.py
â”‚   â”œâ”€â”€ icd_codes.py
â”‚   â””â”€â”€ npi_codes.py
â”œâ”€â”€ ğŸ“ utils/                 # Utility scripts (e.g., CI/CD helpers)
â”‚   â””â”€â”€ add_dags_to_composer.py
â”œâ”€â”€ ğŸ“ workflows/             # Airflow DAGs for orchestration
â”‚   â”œâ”€â”€ bq_dag.py
â”‚   â”œâ”€â”€ parent_dag.py
â”‚   â””â”€â”€ pyspark_dag.py
â”œâ”€â”€ ğŸ“„ cloudbuild.yaml        # GCP CI/CD pipeline configuration
â”œâ”€â”€ ğŸ“„ requirements.txt       # Python environment dependencies
â”œâ”€â”€ ğŸ“„ LICENSE                # Project license
â”œâ”€â”€ ğŸ“„ ProjectNotes.md        # Technical debt and development logs
â””â”€â”€ ğŸ“„ README.md              # Main documentation
```
## Key Features

- Medallion Architecture (Bronze â†’ Silver â†’ Gold)
- PySpark ingestion via Dataproc
- BigQuery transformations with MERGE and SCD logic
- Cloud Composer (Airflow)â€“centric orchestration design
- Automated CI/CD deployment using Cloud Build triggers
- Star schema modeling for analytics
- Modular and production-style DAG structure

---

## Security Considerations

This repository contains sensitive information for demonstration purposes only.  
The project was built as a learning exercise and is not intended for production use.

In real-world scenarios:
- Credentials must be stored in environment variables or Secret Manager
- No passwords or IP addresses should be hardcoded
- IAM roles and least-privilege access should be enforced

---

## Technology Stack

- Python
- PySpark
- SQL
- Google Cloud Storage (GCS)
- Dataproc
- BigQuery
- Cloud Composer (Airflow)
- Cloud SQL
- Cloud Build

---

## Objective

Deliver a scalable, Composer-oriented, production-style data platform that transforms raw healthcare operational data into structured analytical datasets for financial and operational reporting.
